"""
train_xgboost.py
----------------
æœ¬ç¨‹å¼ç”¨æ–¼è¨“ç·´é›»ä¿¡æµå¤±é æ¸¬æ¨¡å‹ï¼ŒåŒ…å«ï¼š
1. åˆå§‹æ¨¡å‹è¨“ç·´
2. XGBoost ç‰¹å¾µé‡è¦æ€§è¨ˆç®—ï¼ˆä»¥ gain ç‚ºæŒ‡æ¨™ï¼‰
3. ç¯©é¸ Gain ç´¯ç©è²¢ç»é” 80% çš„ç‰¹å¾µ
4. ä»¥è©²ç‰¹å¾µé›†é‡æ–°è¨“ç·´æ¨¡å‹ä¸¦é€²è¡Œæ¸¬è©¦é›†è©•ä¼°
"""

# ğŸ“¦ è³‡æ–™è™•ç†èˆ‡è¦–è¦ºåŒ–
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# âš™ï¸ ç‰¹å¾µè™•ç†èˆ‡å»ºæ¨¡æµç¨‹
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# ğŸ“ˆ æ¨¡å‹èˆ‡è¨“ç·´
from xgboost import XGBClassifier

# ğŸ§ª è©•ä¼°æŒ‡æ¨™
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    classification_report
)


# åŒ¯å…¥è¨“ç·´é›†
X_train = pd.read_csv("data/x_origin_train.csv")
y_train = pd.read_csv("data/y_label_è¨“ç·´.csv")
# åŒ¯å…¥é©—è­‰é›†
X_val = pd.read_csv("data/x_origin_val.csv")
y_val = pd.read_csv("data/y_label_é©—è­‰.csv")

# å¦‚æœ y_train æ˜¯ DataFrameï¼Œè½‰ç‚º Series
if isinstance(y_train, pd.DataFrame):
    y_train = y_train.iloc[:, 0]

if isinstance(y_val, pd.DataFrame):
    y_val = y_val.iloc[:, 0]

# ğŸ”¹ å€åˆ†æ¬„ä½é¡å‹
cat_cols = X_train.select_dtypes(include='object').columns.tolist()
num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()

# ğŸ”¹ å»ºç«‹å‰è™•ç†å™¨ï¼ˆOrdinalEncoder for é¡åˆ¥æ¬„ä½ï¼‰
preprocessor = ColumnTransformer([
    ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols)
], remainder='passthrough',
force_int_remainder_cols=False  # ğŸ›¡ï¸ é¿å… future warning
                                 )

# ğŸ”¹ å»ºç«‹ pipelineï¼ˆXGBoost æ¨¡å‹ï¼‰
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('clf', XGBClassifier(
        objective='binary:logistic',
        random_state=42,
        n_estimators=100, learning_rate=0.1, max_depth=5,
      eval_metric='aucpr',scale_pos_weight=3
    ))
])

# ğŸ”¹ è¨“ç·´æ¨¡å‹
pipeline.fit(X_train, y_train)

# ğŸ”¹ é æ¸¬é©—è­‰é›†
y_val_pred = pipeline.predict(X_val)
y_val_prob = pipeline.predict_proba(X_val)[:, 1]  # æ©Ÿç‡åˆ†æ•¸ç”¨æ–¼è¨ˆç®— AUC

# ğŸ”¹ è¨ˆç®—è©•ä¼°æŒ‡æ¨™
val_accuracy = accuracy_score(y_val, y_val_pred)
roc_auc = roc_auc_score(y_val, y_val_prob)
train_accuracy = accuracy_score(y_train, pipeline.predict(X_train))

# ğŸ”¹ å°å‡ºçµæœ
print(f"Train Accuracy: {train_accuracy:.10f}")
print(f"\nValidation Accuracy: {val_accuracy:.10f}")
print(f"ROC AUC: {roc_auc:.10f}\n")

print("Classification Report:")
print(classification_report(y_val, y_val_pred))

print("Confusion Matrix:")
print(confusion_matrix(y_val, y_val_pred))

# å–å¾—è¨“ç·´å¾Œçš„ XGBoost æ¨¡å‹
booster = pipeline.named_steps['clf'].get_booster()

# å–å¾—ç¶“é OrdinalEncoder å¾Œçš„æ‰€æœ‰æ¬„ä½åç¨±
# é¡åˆ¥æ¬„ä½åç¨±
cat_cols = X_train.select_dtypes(include='object').columns.tolist()
num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()
feature_names = cat_cols + num_cols  # OrdinalEncoder ä¸æœƒç”¢ç”Ÿæ–°æ¬„ä½åç¨±

# æŠ“å‡º feature importanceï¼ˆby gainï¼‰
score_dict = booster.get_score(importance_type='gain')

# è½‰ç‚º DataFrameï¼ˆæ³¨æ„ï¼šXGBoost çš„æ¬„ä½åæ˜¯ f0, f1, f2,...ï¼‰
# æŠŠ f0 â feature_names[0]ï¼Œé€™æ¨£å°±èƒ½é¡¯ç¤ºäººé¡å¯è®€çš„ç‰¹å¾µåç¨±ã€‚
importance_df = pd.DataFrame([
    {"feature": feature_names[int(f[1:])], "gain": score}
    for f, score in score_dict.items()
])

# æ’åº + è¨ˆç®—ç´¯ç©è²¢ç»æ¯”ä¾‹
importance_df = importance_df.sort_values(by="gain", ascending=False).reset_index(drop=True)
importance_df["gain_pct"] = importance_df["gain"] / importance_df["gain"].sum()
importance_df["gain_cumsum"] = importance_df["gain_pct"].cumsum()

#ï¼ˆé¸é…ï¼‰ç•«åœ–å‰ 20 å
plt.figure(figsize=(10, 6))
plt.barh(importance_df.head(20)['feature'][::-1], importance_df.head(20)['gain'][::-1])
plt.xlabel("Gain")
plt.title("Top 20 Features by Gain")
plt.tight_layout()
plt.show()

# é¸å‡º Gain 80% çš„ç‰¹å¾µ
# å–å¾— Gain 80% ç‰¹å¾µ
# å»ºç«‹ Gain 80% çš„ç‰¹å¾µæ¸…å–®
selected_features_80 = importance_df[importance_df["gain_cumsum"] <= 0.80]["feature"].tolist()

print(f"ğŸ¯ Gain 80% ç‰¹å¾µæ•¸é‡ï¼š{len(selected_features_80)}")
print(selected_features_80)

# ğŸ” retrain å‡½æ•¸ï¼ˆä»¥ä»»æ„ç‰¹å¾µæ¸…å–®ï¼‰
def retrain_and_evaluate(feature_subset, subset_name):
    print(f"\nğŸ” Retrain Model using {subset_name} ({len(feature_subset)} features)\n")

    # ğŸ”¹ ç¯©é¸ç‰¹å¾µå­é›†
    X_train_sel = X_train[feature_subset].copy()
    X_val_sel   = X_val[feature_subset].copy()

    # ğŸ”¹ å€åˆ†æ¬„ä½é¡å‹
    cat_cols = X_train_sel.select_dtypes(include='object').columns.tolist()

    # ğŸ”¹ å‰è™•ç†å™¨ï¼šOrdinal ç·¨ç¢¼
    preprocessor = ColumnTransformer([
        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), cat_cols)
    ], remainder='passthrough',force_int_remainder_cols=False)

    # ğŸ”¹ Pipelineï¼ˆå« XGBoost æ¨¡å‹ï¼‰
    pipeline = Pipeline([
        ('preprocess', preprocessor),
        ('clf', XGBClassifier(
            objective='binary:logistic',
            eval_metric='aucpr',
            random_state=42,
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            scale_pos_weight=3
        ))
    ])

    # ğŸ”¹ è¨“ç·´æ¨¡å‹
    pipeline.fit(X_train_sel, y_train)

    # ğŸ”¹ é æ¸¬èˆ‡è©•ä¼°
    y_val_pred = pipeline.predict(X_val_sel)
    y_val_prob = pipeline.predict_proba(X_val_sel)[:, 1]
    y_train_pred = pipeline.predict(X_train_sel)

    print(f"Train Accuracy: {accuracy_score(y_train, y_train_pred):.10f}")
    print(f"Validation Accuracy: {accuracy_score(y_val, y_val_pred):.10f}")
    print(f"ROC AUC: {roc_auc_score(y_val, y_val_prob):.10f}\n")
    print("Classification Report:")
    print(classification_report(y_val, y_val_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_val, y_val_pred))
    return pipeline, X_train_sel, X_val_sel

# âœ… åŸ·è¡Œ Gain 80% ç‰¹å¾µç‰ˆæœ¬
pipeline_80, X_train_sel, X_val_sel = retrain_and_evaluate(selected_features_80, "Gain 80%")

# æ¸¬è©¦é›†çµæœ
X_test = pd.read_csv("data/x_origin_test.csv")
y_test = pd.read_csv("data/y_label_test.csv")  # å¦‚æœæœ‰æ¨™ç±¤çš„è©±

# å¦‚æœ y_test æ˜¯ DataFrameï¼Œè½‰ç‚º Series
if isinstance(y_test, pd.DataFrame):
    y_test = y_test.iloc[:, 0]

# é¸æ“‡ Gain 80% ç‰¹å¾µ
X_test_sel = X_test[selected_features_80].copy()

# æ©Ÿç‡é æ¸¬
y_test_proba = pipeline_80.predict_proba(X_test_sel)[:, 1]

# è‡ªè¨‚ threshold çš„é æ¸¬
y_test_pred_custom = (y_test_proba >= 0.5).astype(int)

print(f"âœ… Test Accuracy: {accuracy_score(y_test, y_test_pred_custom):.4f}")
print(f"âœ… ROC AUC: {roc_auc_score(y_test, y_test_pred_custom):.4f}")
print(f"ğŸ“Œ Precision (class=1): {precision_score(y_test, y_test_pred_custom):.4f}")
print(f"ğŸ“Œ Recall (class=1): {recall_score(y_test, y_test_pred_custom):.4f}")
print(f"ğŸ“Œ F1-score (class=1): {f1_score(y_test, y_test_pred_custom):.4f}")

print("\nğŸ“Š Classification Report:")
print(classification_report(y_test, y_test_pred_custom))
print("ğŸ“‰ Confusion Matrix:")
print(confusion_matrix(y_test, y_test_pred_custom))

import pickle

# ğŸ“Œ å„²å­˜æœ€çµ‚æ¨¡å‹ï¼ˆpipeline_80ï¼‰ç‚º pkl æª”
with open("model/xgb_pipeline_gain80.pkl", "wb") as f:
    pickle.dump(pipeline_80, f)
